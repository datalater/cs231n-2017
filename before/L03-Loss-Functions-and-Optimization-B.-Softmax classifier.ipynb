{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ⓒ JMC 2017\n",
    "\n",
    "**Last update:** 2017.10.20\n",
    "\n",
    "**Email:** the7mincheol@gmail.com  \n",
    "**Github:** github.com/datalater  \n",
    "**Facebook:** fb.com/datalater  \n",
    "\n",
    "\n",
    "**Reference**  \n",
    "\\- cs231n (2017) https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Softmax function이 하는 일\n",
    "\n",
    "multi-class SVM의 모델 $f(x,W)$이 뱉어내는 각 class에 대한 score 값은 true class에 대한 score가 incorrect class에 대한 score보다 높으면 좋은 모델로 삼도록 loss function을 정의하도록 사용된다.\n",
    "하지만 score 값 자체가 어떤 의미인지는 알 수 없었다.\n",
    "가령, input 이미지를 넣었을 때 class cat에 대한 score가 3.2이고 class car에 대한 score가 5.1이라면 input 이미지가 car에 속할 가능성이 더 높다는 것은 알겠지만 3.2나 5.1이 가지는 의미가 무엇인지는 해석할 수가 없었다.\n",
    "softmax classifier(=multinomial logistic regression)는 score를 \"확률\"값 $P$로 바꿔준다.\n",
    "\n",
    "![L03-softmax2.png](images/L03-softmax2.png)\n",
    "\n",
    "특정 이미지를 입력 받은 후 각 class에 대한 score 값을 exponentiate해서 양의 값으로 만든다.\n",
    "그리고 각 exponent를 모든 exponent의 합으로 나눠준다.\n",
    "이러한 일련의 과정을 거쳐서 unnormalized log probabilities였던 scores가 probabilities로 바뀐다.\n",
    "데이터가 특정 class에 속할 확률값을 출력하는 함수를 softmax function이라고 부른다.\n",
    "score 값을 각 class에 대한 probability distribution, 즉 $P$로 바꿨다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Softmax의 loss와 loss function 정의\n",
    "\n",
    "![L03-softmax3.png](images/L03-softmax3.png)\n",
    "\n",
    "loss를 정의하려면 $P$와 target probability distribution을 비교해야 한다. target probability distribution을 살펴보면, true class에 대한 probability mass는 1이 되고 나머지 class에 대한 probability mass는 0이 된다.\n",
    "우리가 원하는 것은 $P$가 target probability distribution과 최대한 가까워지도록 만드는 것이다.\n",
    "\n",
    "> **Note**: $P$ : computed probability distribution\n",
    "\n",
    "$P$가 target probability distribution과 가까워지게 만드는 방법은 여러 가지가 있다.\n",
    "target probability distribution과 $P$(computed probability distribution) 사이의 KL-divergence를 구한다거나, 또는 MLE(최우추정법)을 사용할 수도 있다.\n",
    "방법이 무엇이 됐든, 우리가 원하는 최종 형태는 true class에 대한 probability가 1에 가까워야 한다는 것이다.\n",
    "그런데 다음과 같은 2가지 이유로 loss function의 형태는 negative log 형태가 된다.\n",
    "\n",
    "첫째, log 형태가 되는 이유는 수학적으로 raw probability를 maximize하는 것보다 log log를 maxmize하는 것이 더 쉽기 때문이다.\n",
    "둘째, negative를 취하는 이유는 ture class에 대한 $\\log P$를 maximize하게 되면 나쁜 정도가 아니라 좋은 정도를 측정하기 때문이다.\n",
    "loss는 파라미터의 나쁜 정도를 측정해야 하므로 negative를 취해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) negative log로 정의되는 loss function의 직관적인 의미\n",
    "\n",
    "loss function을 직관적으로 이해하려면 극단적인 값을 넣어보면 된다.\n",
    "true class에 대한 $P$값이 0이 되었다고 하자.\n",
    "true class에 속할 확률이 0이라는 것은 최악의 파라미터이므로 loss를 maximum으로 내뱉어야 한다.\n",
    "$L = -\\log P= -(-\\infty) = \\infty$가 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) hinge loss (SVM) vs. cross-entropy loss (Softmax)\n",
    "\n",
    "![L03-softmax.vs.SVM.png](images/L03-softmax.vs.SVM.png)\n",
    "\n",
    "SVM의 hinge loss는 correct score가 incorrect score보다 특정 margin보다 커지도록 신경쓴다.\n",
    "그러나 Softmax의 cross-entropy loss는 correct probability가 반드시 1이 되도록 신경쓴다.\n",
    "score 관점에서 보면, Softmax는 correct class에 대해서는 plus infinity score를 주려고 하고 incorrect class에 대해서는 minus infiinity score를 주려고 한다.\n",
    "\n",
    "> **Note**: softmax 알고리즘의 loss를 cross-entropy loss라고 부른다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Recap\n",
    "\n",
    "![L03-lossFunctionsRecap.png](images/L03-lossFunctionsRecap.png)\n",
    "\n",
    "loss를 정의하는 여러 가지 방법 중에서 Softmax의 cross-entropy loss와 SVM의 hinge loss를 알아보았다.\n",
    "data에 대한 loss를 정의한 이후에는 Regularization loss를 추가해서 Full loss function을 완성한다.\n",
    "\n",
    "**끝.**\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
