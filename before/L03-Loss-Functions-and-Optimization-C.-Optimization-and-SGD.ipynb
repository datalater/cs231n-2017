{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ⓒ JMC 2017\n",
    "\n",
    "**Last update:** 2017.10.20\n",
    "\n",
    "**Email:** the7mincheol@gmail.com  \n",
    "**Github:** github.com/datalater  \n",
    "**Facebook:** fb.com/datalater  \n",
    "\n",
    "\n",
    "**Reference**  \n",
    "\\- cs231n (2017) https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Optimization\n",
    "\n",
    "loss function을 정의하는 방법까지는 알겠는데, 실제로 loss를 최소화하는 $W$를 어떻게 찾을 수 있을까?\n",
    "optimization을 직관적으로 이해하려면 등산하러 온 사람이 집으로 돌아가기 위해 산 정상에서 가장 낮은 곳으로 내려와야 하는 상황을 떠올리면 된다.\n",
    "이때 좌우로 움직일 때마다 변하는 좌표는 $W$를 의미하고, 좌표에 따라 낮아지거나 높아지는 산의 높이는 loss를 의미한다.\n",
    "\n",
    "모양이 단순한 산을 내려오는 일은 어려운 일이 아니지만, 모델 함수 $f$와 loss function, 그리고 regularizer 모두 매우 크고 복잡해진다면 minima에 다다르기 위한 명확한 해석적 방법(explicit analytic solution)을 찾기란 거의 불가능하다.\n",
    "그래서 실전에서는 여러 가지 반복적 방법을 사용한다.\n",
    "반복적 방법이란, 어떤 solution에서 시작하여 solution을 점점 더 향상시키는 방법을 뜻한다.\n",
    "\n",
    "떠올릴 수 있는 방법 한 가지는, 현재 위치에서 가장 낮은 곳으로 가는 길이 보이지 않더라도 발걸음을 옮겨 가면서 더 낮은 곳으로 가는 방향이 어딘지 살펴보는 것이다.\n",
    "즉, 경사가 낮아지는 곳이 어딘지 살펴보고 그 방향으로 이동하는 것을 반복하는 방법이 있다.\n",
    "이러한 알고리즘은 매우 간단하지만 neural network나 linear classifier 등을 사용하는 실전에서 매우 잘 작동한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![L03-optimization-gradient.png](images/L03-optimization-gradient.png)\n",
    "\n",
    "경사(slope)란 무엇일까?\n",
    "1차원 공간에서 경사는 1차원 함수를 미분한 스칼라 값을 말한다.\n",
    "우리가 다루는 input 벡터 $x$와 파라미터 벡터 $w$는 다차원이다.\n",
    "다차원 공간에서 경사(gradient)는 다차원 함수를 편미분한 편미분 벡터를 말한다.\n",
    "편미분 벡터의 각 요소는 그 방향으로 움직일 경우 함수의 경사가 어떻게 되는지 말해준다.\n",
    "다시 말하면 편미분 벡터 gradient는 함수의 값이 가장 커지는 방향이 어딘지를 가리키고 있다.\n",
    "따라서 gradient 벡터의 반대 방향으로 가면 함수의 값이 가장 작아지는 방향이 된다.\n",
    "만약 다차원 공간에서 특정 방향에 대한 경사를 알고 싶다면, 특정 방향을 가진 unit vector에 gradient vector를 내적(dot product)하면 된다.\n",
    "gradient가 매우 중요한 이유는 현재 지점에서 모델 함수의 선형 일차 근사(linear, first-order approximation)를 제공하기 때문이다.\n",
    "실제로 딥러닝을 사용하는 수많은 경우에서 모델 함수의 gradient를 계산한 후 gradient를 사용해서 파라미터 벡터 $w$를 반복적으로 업데이트한다.\n",
    "\n",
    "> **Note**: 우리말로 slope나 gradient나 모두 경사를 뜻하지만, 다차원 공간의 경사(slope)는 따로 gradient라고 지칭한다. gradient는 벡터 값이므로 gradient vector라고도 말한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Gradient Descent\n",
    "\n",
    "![L03-optimization-gradient-descent.png](images/L03-optimization-gradient-descent.png)\n",
    "\n",
    "gradient를 구할 줄 알면, 엄청나게 크고 가장 복잡한 딥러닝 알고리즘도 아주 쉽게 학습할 수 있다.\n",
    "gradient descent 알고리즘은 먼저 파라미터 $W$를 임의 값으로 초기화한 다음, loss와 gradient를 계산해서 gradient의 반대 방향으로 파라미터 $W$를 업데이트한다.\n",
    "앞서 말했듯이 gradient는 함수의 값이 가장 커지는 방향을 가리키고 있고, 우리는 loss function의 값을 줄여야 하기 때문에 gradient 반대 방향으로 업데이트 해야 한다.\n",
    "이렇게 gradient 반대 방향으로 한 스텝씩 이동하는 것을 꾸준히 반복하면, 모델이 어느 지점으로 수렴하게 될 것이다.\n",
    "여기서 스텝 사이즈는 hyperparameter이다.\n",
    "스텝 사이즈는 gradient를 계산할 때마다 gradient 반대 방향으로 얼마나 멀리 이동할지를 뜻한다.\n",
    "스텝 사이즈는 learning rate라고도 불리며, 연구자가 데이터를 학습할 때 고려해야 하는 가장 중요한 hyperparameter 중 하나이다.\n",
    "강의자의 경우 적절한 스텝 사이즈를 알아내는 것이 가장 먼저 체크하는 hyperparameter라고 한다.\n",
    "모델의 크기나 regularization strength 등 여러 가지 hyperparameter가 있지만 스텝 사이즈를 가장 먼저 체크한다고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![L03-optimization-gradient-descent-visual.png](images/L03-optimization-gradient-descent-visual.png)\n",
    "\n",
    "2차원 공간으로 loss function이 표현된 예제에서 gradient descent 알고리즘이 어떻게 작동하는지 보자.\n",
    "가운데 빨간 영역은 loss가 가장 낮은 영역으로써 우리가 도달하고 싶은 곳이다.\n",
    "가장자리의 파란색과 보라색 영역은 loss가 높은 영역으로 우리가 피하고 싶은 곳이다.\n",
    "gradient descent 알고리즘을 실행하면, 먼저 파라미터 W를 공간 상에 임의의 지점으로 시작한다.\n",
    "그리고 빨간색 영역에 있는 minima로 도달하도록 매 스텝마다 negative gradient direction을 계산해서 이동한다.\n",
    "\n",
    "gradient descent의 기본 원리는 매 스텝마다 gradient를 사용해서 다음 스텝으로 어디로 이동할지를 결정해서 매 스텝마다 내리막으로 이동하는 것이다.\n",
    "그런데 gradient를 어떻게 사용할지에 대한 update rules에는 다양한 방법이 존재한다.\n",
    "update rules에 다양한 방법이 존재하는 이유는 기본적인 gradient descent 알고리즘, 즉 Vanilla Gradient Descent에는 약점이 있기 때문이다.\n",
    "\n",
    "> **Note**: 기계학습에서 \"vanilla\"라는 용어가 자주 등장하는데 이는 '평범한, 기본적인'이라는 뜻이다. 즉, Vanila Gradient Descent는 우리가 처음 배운 gradient descent와 동일한 개념이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이전에 loss를 정의해서 우리가 만든 classifier가 각각의 training example 하나마다 어떤 오차가 있는지 계산했다.\n",
    "그리고 loss function을 정의해서 training dataset 전체에 대한 loss를 평균 내서 full loss를 계산했다.\n",
    "그런데 실제로 training data의 수 $N$은 매우 매우 커질 수 있다.\n",
    "ImageNet의 데이터셋을 사용하면 $N$의 크기는 130만 개이다.\n",
    "따라서 모든 training dataset을 전부 활용하는 Vanila Gradient Descent 알고리즘에서는 loss와 gradient를 계산하는 것이 매우 매우 비싸고 느릴 수밖에 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Stochastic Gradient Descent and Minibatch\n",
    "\n",
    "![L03-optimization-gradient-descent-SGD.png](images/L03-optimization-gradient-descent-SGD.png)\n",
    "\n",
    "여기서 우리가 논의해야 할 좋은 묘안이 하나 있다.\n",
    "위 슬라이드를 보자.\n",
    "gradient는 선형 연산(linear operator)이기 때문에 함수식 $L(W)$에 대한 gradient를 계산해보면 loss에 대한 gradient인 $\\Sigma_{i=1}{N} \\triangledown_{W}L_{i}(x_i, y_i, W)$는 loss의 gradient를 모두 합한 것이 된다.\n",
    "따라서 gradient를 한 번 더 계산할 때마다, training data 전체에 대해 한 번 더 계산해야 한다.\n",
    "$N$이 백만 단위를 넘어가면 gradient 연산이 매우 오래 걸리게 되는데, 이는 결국 파라미터 $W$를 한 번 업데이트 하기 위해서 엄청나게 긴 시간을 기달려야 한다는 뜻이 된다.\n",
    "그래서 실전에서는 Stochastic Gradient Descent (SGD)라고 불리는 알고리즘을 사용한다.\n",
    "SGD는 loss와 gradient를 계산할 때 모든 training set 전체를 사용하는 것이 아니라, traing set 중에서 샘플링한 몇 개의 데이터(=minibatch)를 사용한다.\n",
    "\n",
    "> **Note**: 슬라이드의 코드를 보면 data_batch를 구할 때 전체 training set에서 256개 데이터를 샘플링하고 있다. 이렇게 training set에서 샘플링된 데이터가 바로 minibatch이다. minibatch의 개수는 일반적으로(by convention) 32, 64, 128처럼 2의 거듭제곱으로 사용한다.\n",
    "\n",
    "즉, SGD는 minibatch를 사용해서 true full loss와 true gradient의 추정치(estimate)를 구하는 것이다.\n",
    "이렇게 모수에 대한 추정치를 사용하는 측면이 확률적인 속성이므로 stochastic하다고 말한다.\n",
    "따라서 SGD는 minibatch를 사용해서 loss와 gradient를 계산한 후 파라미터 $W$를 업데이트 한다.\n",
    "\n",
    "**끝.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
