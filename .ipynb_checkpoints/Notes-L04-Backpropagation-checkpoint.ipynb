{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ⓒ JMC 2017\n",
    "\n",
    "+ [CS231N Video - Playlist](https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv)  \n",
    "+ [CS231N Notes - Backpropgation](http://cs231n.github.io/optimization-2/)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem statement**:\n",
    "\n",
    "우리가 풀어야 할 문제는 함수 $f(x)$에서 input $x$가 함수값 $f$에 미치는 영향을 구하는 것이다.\n",
    "함수값에 대한 변수의 영향력을 gradient라고 하며 $\\triangledown f(x)$라고 표기한다.\n",
    "\n",
    "+ function: $f(x)$\n",
    "+ input: $x$\n",
    "+ gradient of $f$ at $x$: $\\triangledown f(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Motivation**:\n",
    "\n",
    "성능이 뛰어난 Neural Network 모델을 만들려면 loss를 최소화하는 parameter를 구해야 한다.\n",
    "parameter를 어떻게 변화시켜야 loss가 줄어들까.\n",
    "parameter값이 loss function에 미치는 영향(=변화율)인 gradient를 구하면 해결할 수 있다.\n",
    "\n",
    "+ function: $f(x) = L$(loss function)\n",
    "+ input: $x$\n",
    "  1. training data: $(x_i, y_i)$\n",
    "  2. weights: $W, b$\n",
    "+ gradient:\n",
    "  1. gradient of $f$ at training data $(x_i, y_i)$ = $\\triangledown L(x_i, y_i)$\n",
    "  2. gradient of $f$ at weights $(W, b)$ = $\\triangledown L(W, b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Simple expression and interpretation of the gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient = partial derivative**:\n",
    "\n",
    "+ function : $f(x, y) = xy$\n",
    "+ input : $x, y$\n",
    "+ gradient:\n",
    "  1. gradient of $f$ at $x$ = partial derivative for $x$ = $y$\n",
    "  2. gradient of $f$ at $y$ = partial derivative for $y$ = $x$\n",
    "+ gradient:\n",
    "  + $\\triangledown f$ = vector of partial derivatives\n",
    "  + $\\triangledown f$ = $[\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y}]$\n",
    "  + $\\triangledown f$ = $[y, x]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**:\n",
    "\n",
    "+ derivative 의미 : 특정 변수가 함수값에 미치는 영향(=변화율)\n",
    "+ $f(x+h)=f(x)+h\\frac{df(x)}{dx}$\n",
    "\n",
    "> **Tells**: $x$가 $h$만큼 증가하면, $f$값은 \"$x$의 gradient 값$ \\times h = h\\frac{df(x)}{dx}$\"만큼 증가한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**:\n",
    "\n",
    "(1) Multiplication\n",
    "\n",
    "+ $f(x,y) = xy = -12$\n",
    "+ $x = 4, y=-3$\n",
    "+ gradient on $x$ (=partial derivative on $x$) : $\\frac{\\partial f}{\\partial x} = y = -3$\n",
    "+ gradient on $y$ (=partial derivative on $y$) : $\\frac{\\partial f}{\\partial y} = x = 4$\n",
    "\n",
    "> **Tells**: $x$값이 $h$만큼 증가하면, $f$값은 \"$x$의 gradient 값$ \\times h = h\\frac{df(x)}{dx}=hy=-3h$\"만큼 감소한다. $y$값이 $h$만큼 증가하면, $f$값은 \"$y$의 gradient 값$ \\times h = h\\frac{df(y)}{dy}=hx=4h$\"만큼 증가한다.\n",
    "\n",
    "(2) Addition\n",
    "\n",
    "+ $f(x,y)=x+y$\n",
    "+ gradient on $x$ : $\\frac{\\partial f}{\\partial x} = 1$\n",
    "+ gradient on $y$ (=partial derivative on $y$) : $\\frac{\\partial f}{\\partial y} = 1$\n",
    "\n",
    "> **Tells**: $x$값이 $h$만큼 증가하면, $f$값은 \"$x$의 gradient 값$ \\times h = h\\frac{df(x)}{dx}=h$\"만큼 (1:1비율로) 증가한다. $y$값이 $h$만큼 증가하면, $f$값은 \"$y$의 gradient 값$ \\times h = h\\frac{df(y)}{dy}=h$\"만큼 (1:1비율로) 증가한다.\n",
    "\n",
    "(3) Max\n",
    "\n",
    "+ $f(x,y)=max(x,y)$\n",
    "+ gradient on $x$\n",
    "  + if, $x \\ge y \\Rightarrow \\frac{\\partial f}{\\partial x} = 1$\n",
    "  + if, $x < y \\Rightarrow \\frac{\\partial f}{\\partial x} = 0$\n",
    "+ gradient on $y$\n",
    "  + if, $y \\ge x \\Rightarrow \\frac{\\partial f}{\\partial y} = 1$\n",
    "  + if, $y < x \\Rightarrow \\frac{\\partial f}{\\partial y} = 0$\n",
    "+ gradient $\\triangledown f$ = $[\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y}]$\n",
    "\n",
    "> **Tells**: 더 큰 input값의 gradient는 1이다. input이 $h$만큼 증가하면, $f$값은 \"input의 gradient 값$ \\times h = h\\frac{df(input)}{dinput}=h$\"만큼 (1:1비율로) 증가한다.\n",
    "\n",
    "> **Tells**: 더 작은 input값의 gradient는 0이다.\n",
    "$input$값이 $h$만큼 증가하면, $f$값은 \"$input$의 gradient 값$ \\times h = h\\frac{df(input)}{dinput}=0$\"만큼 증가한다. 즉, $input$값의 변화는 $f$값에 아무런 영향을 미치지 않는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Compound expressions with chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Complicated expressions**:\n",
    "\n",
    "+ function : $f(x, y, z) = (x+y)z$\n",
    "+ gradient\n",
    "  1. $\\frac{df}{dx}$\n",
    "  2. $\\frac{df}{dy}$\n",
    "  3. $\\frac{df}{dz}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Break down into multiplication**:\n",
    "\n",
    "+ $f = (x+y)z$\n",
    "+ $q = x + y$\n",
    "+ $f = qz$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute derivatives separately**:\n",
    "\n",
    "+ $f = qz$\n",
    "  + $\\frac{\\partial f}{\\partial q} = z$ : chain의 연결고리로 사용된다.\n",
    "  + $\\frac{\\partial f}{\\partial z} = q$ : gradient 3.\n",
    "+ $q = x + y$\n",
    "  + $\\frac{\\partial q}{\\partial x} = 1$ : chain의 연결고리로 사용된다.\n",
    "  + $\\frac{\\partial q}{\\partial y} = 1$ : chain의 연결고리로 사용된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient 1 & 2**:\n",
    "\n",
    "+ gradient 1 : $\\frac{df}{dx} = \\frac{\\partial f}{\\partial q} \\cdot \\frac{\\partial q}{\\partial x}$\n",
    "+ gradient 2 : $\\frac{df}{dy} = \\frac{\\partial f}{\\partial q} \\cdot \\frac{\\partial q}{\\partial y}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example code**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dfdx: -4 \n",
      "dfdy: -4 \n",
      "dfdz: 3\n"
     ]
    }
   ],
   "source": [
    "# set some inputs\n",
    "x = -2; y = 5; z = -4;\n",
    "\n",
    "# perform the forward pass\n",
    "q = x + y  # q becomes 3\n",
    "f = q * z  # f becomes -12\n",
    "\n",
    "# perform the backward pass (backpropagation) in reverse order:\n",
    "# first backprop through f = q * z\n",
    "dfdz = q  # df/dz = q, so gradient on z becomes 3\n",
    "dfdq = z  # df/dq = z, so gradient on q becomes -4\n",
    "\n",
    "# dqdx and dqdy\n",
    "dqdx = 1.0\n",
    "dqdy = 1.0\n",
    "\n",
    "# now backprop through q = x + y\n",
    "dfdx = dfdq * dqdx  # dfdx = 1.0. This multiplication here is the chain rule!\n",
    "dfdy = dfdq * dqdy  # dfdy = 1.0\n",
    "\n",
    "print(\"dfdx: %d \\ndfdy: %d \\ndfdz: %d\" %(dfdx, dfdy, dfdz))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV. Intuitive understanding of backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![cs231n_2017_lecture4-backpropagation-nerualNetworks_06.png](images/cs231n_2017_lecture4-backpropagation-nerualNetworks_06.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**local**:\n",
    "\n",
    "전체 graph와 별개로 개별 연산 node에서 output과 local gradient를 구한다.\n",
    "\n",
    "+ (1) compute output : $f(x, y) = z$\n",
    "+ (2) compute local gradient of its output at its inputs: $\\left[\\frac{\\partial z}{\\partial x}, \\frac{\\partial z}{\\partial y}\\right]$\n",
    "\n",
    "> **Note**: local gradient = gradient of its output at its inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**one forward pass, one backward pass**\n",
    "\n",
    "forward pass가 앞으로 한 번 발생하면 개별 연산 node에 input이 들어가고 전체 graph의 final output이 나온다.\n",
    "이때 backward pass로 뒤로 거슬러 올 때 local에서 구한 local gradient와 final output의 gradient를 chain하면 개별 연산 node의 input에 대한 final output의 gradient를 구할 수 있다.\n",
    "\n",
    "+ (3) compute gradient of final output at its output: $\\frac{\\partial L}{\\partial z}$\n",
    "+ (4) chain gradients: gradient $\\times$ local gradient\n",
    "  + $= \\left[\\frac{\\partial L}{\\partial z} \\times \\frac{\\partial z}{\\partial x}, \\frac{\\partial L}{\\partial z} \\times \\frac{\\partial z}{\\partial y}\\right]$\n",
    "  + $= \\left[\\frac{\\partial L}{\\partial x}, \\frac{\\partial L}{\\partial y}\\right]$\n",
    "\n",
    "> **Note**: (4) gradient = gradient of final output its inputs = (2) $\\times$ (3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![cs231n_2017_lecture4-backpropagation-nerualNetworks_08.png](images/cs231n_2017_lecture4-backpropagation-nerualNetworks_08.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V. Modularity: Sigmoid example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`@@@resume`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
