{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ⓒ JMC 2017\n",
    "\n",
    "**Last update:** 2017.10.20\n",
    "\n",
    "**Email:** the7mincheol@gmail.com  \n",
    "**Github:** github.com/datalater  \n",
    "**Facebook:** fb.com/datalater  \n",
    "\n",
    "\n",
    "**Reference**  \n",
    "\\- cs231n (2017) https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이하 등장하는 내용은 딥뉴럴넷이 등장하기 전 이미지의 feature를 활용해서 linear classifier의 성능을 향상시킨 이야기를 먼저 하고 이후에 딥뉴럴넷과 비교한다.\n",
    "\n",
    "### 1) Image Features\n",
    "\n",
    "![L03-image-features.png](images/L03-image-features.png)\n",
    "\n",
    "지금까지 linear classifier에 대해 논의했다.\n",
    "linear classifier는 이미지의 픽셀값을 그대로 입력 받는데, 실제로 이런 linear classifier는 잘 작동하지 않는다.\n",
    "그래서 2단계로 접근하는 대안이 등장했다.\n",
    "첫째, input 이미지의 특징이 될 만한 여러 가지 feature representation(특징 표현)을 계산한다.\n",
    "둘째, 여러 가지 feature 벡터를 결합한 feature representation을 linear classifier에 입력한다.\n",
    "그러니까 이전에는 이미지의 raw pixel을 입력했다면, 이후에는 선별적으로 고른 feature pixel을 입력한 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![L03-image-features-motivation.png](images/L03-image-features-motivation.png)\n",
    "\n",
    "이전 강의에서도 설명했듯이 슬라이드 왼쪽과 같이 데이터가 나뉘어 있는 경우, linear classifier로는 빨간색 데이터 무리와 파란색 데이터 무리를 분리해낼 수 없었다.\n",
    "그런데 feature transform을 사용하면, 빨간색 데이터와 파란색 데이터가 linearly separable하게 변형된다.\n",
    "즉, 이미지에서 올바른 feature transform을 할 수 있다면 linear classifier의 성능을 향상시킬 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![L03-image-features-color-histogram.png](images/L03-image-features-color-histogram.png)\n",
    "\n",
    "feature transform의 간단한 예시 중 하나가 color histogram이다.\n",
    "hue spectrum을 몇 개의 구획으로 나눈 후, input 이미지의 각 픽셀에 해당하는 hue를 spectrum의 구획에 맵핑해서 각 구획을 기준으로 하는 histogram으로 나타낸 것이다.\n",
    "이러한 color histogram은 input 이미지에 어떤 color가 있는지 말해준다.\n",
    "input 이미지가 슬라이드처럼 개구리 이미지라면 histogram 분포에서 녹색이 많을 것이고 빨간색이나 보라색은 적을 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![L03-image-features-HoG.png](images/L03-image-features-HoG.png)\n",
    "\n",
    "또 다른 예시는 HoG라고 불리는 Histogram of Oriented Gradients이다.\n",
    "HoG의 기본 원리는 이미지의 edge에 대한 local orientation을 측정하는 것이다.\n",
    "HoG의 작동 방식은 다음과 같다.\n",
    "먼저 input 이미지를 입력 받은 다음 8 by 8 픽셀 영역으로 나눈다.\n",
    "그리고 각 8 by 8 픽셀 영역 안에서 dominant edge direction이 무엇인지 계산하고, 계산한 edge direction을 histogram의 구획으로 수치화한다.\n",
    "그리고 각 픽셀 영역 안에서 서로 다른 edge direction에 대한 histogram을 계산한다.\n",
    "즉, HoG는 edge information의 종류가 어떻게 존재하는지 말해준다.\n",
    "슬라이드 왼쪽의 개구리 이미지를 HoG를 사용해서 feature vector를 구하고 그림으로 나타내면 슬라이드의 오른쪽과 같이 표현된다.\n",
    "나뭇잎이 가진 오른쪽으로 하강하는 대각선이 feature representation에서 잘 표현되어 있다.\n",
    "이러한 feature representation은 매우 흔하게 사용된 방법으로 object recognition에서 자주 사용되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![L03-image-features-Bag-of-Words.png](images/L03-image-features-Bag-of-Words.png)\n",
    "\n",
    "또 다른 feature representation의 예시는 Bag of Words이다.\n",
    "NLP에서 영감을 받은 아이디어이다.\n",
    "단락을 표현하는 feature vector를 어떻게 만들 수 있을까?\n",
    "단락에 등장한 단어의 개수를 세서 벡터로 나타내면 feature vector로 사용할 수 있다.\n",
    "문제는 단락에서는 단어로 단락을 나타낼 수 있었지만, 이미지에서는 이미지를 나타낼 단어의 역할을 할만한 개념이 딱히 없다는 것이다.\n",
    "따라서 이미지를 나타낼 visual words(=vocabulary)를 정의해야 한다.\n",
    "\n",
    "2단계 접근법을 사용한다.\n",
    "수많은 이미지를 모은 후, 각 이미지를 조각으로 잘라서 그 조각의 모음 중에서 샘플링을 한다.\n",
    "그리고 샘플링한 조각 이미지를 K-means 같은 알고리즘을 사용해서 clustering 한다.\n",
    "clustering 되면 cluster의 center는 서로 다른 종류의 visual words를 대표하는 값이 된다.\n",
    "슬라이드 step1에서 오른쪽을 보면 서로 다른 color와 edge orientation을 나타내는 visual words들이 표현되어 있다.\n",
    "step2에서는 input 이미지를 넣고, 이 input 이미지에서 visual words가 얼마나 많이 발생하는지를 기준으로 encode한다.\n",
    "즉, 이미지의 visual appearance를 측정한 것이므로 feature representation이 되는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Image Features vs. ConvNets\n",
    "\n",
    "![L03-image-features-vs-convnets.png](images/L03-image-features-vs-convnets.png)\n",
    "\n",
    "지금까지 얘기한 image features를 종합해서 convolutional network와 비교해보겠다.\n",
    "이미지 분류 문제의 해결방법은 슬라이드와 같이 요약된다.\n",
    "첫 번째 방법은, Bag of Words나 HoG를 사용해서 이미지의 feature representation을 계산한 다음 모든 feature를 결합한 것을 linear classifier에 넣는다.\n",
    "여기서 사용된 feature extractor는 고정된 것으로 학습하는 동안 업데이트 되지 않는다.\n",
    "두 번째 방법인 ConvNets으로 오면 feature를 미리 정하기 전에 feature를 데이터로부터 직접 배운다는 게 가장 큰 차이점이다.\n",
    "ConvNets은 이미지의 raw pixel을 그대로 입력 받고 네트워크의 수많은 layer를 통해 계산된 다음 데이터에 기반하여 feature representation을 하고 모든 파라미터를 업데이트한다.\n",
    "즉, 첫 번째 방법처럼 linear classifier의 파라미터만 업데이트하는 것이 아니라 네트워크에 있는 모든 파라미터를 업데이트한다.\n",
    "\n",
    "**끝.**\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
