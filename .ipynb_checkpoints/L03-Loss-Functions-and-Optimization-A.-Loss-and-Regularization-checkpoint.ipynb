{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ⓒ JMC 2017\n",
    "\n",
    "**Last update:** 2017.10.20\n",
    "\n",
    "**Email:** the7mincheol@gmail.com  \n",
    "**Github:** github.com/datalater  \n",
    "**Facebook:** fb.com/datalater  \n",
    "\n",
    "\n",
    "**Reference**  \n",
    "\\- cs231n (2017) https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Loss\n",
    "\n",
    "loss란 우리가 만든 classifier가 얼마나 틀렸는지를 측정하는 개념이다.\n",
    "loss 값은 loss function 공식을 정의해서 구한다.\n",
    "이때 loss를 어떻게 정의하느냐에 따라 loss function으로 여러 가지 공식을 사용할 수 있다.\n",
    "classifier가 저지르는 실수가 여러 종류라면, 그 중 유난히 조심해야 하는 실수가 있을 수 있다.\n",
    "예를 들어, 법학에서 유죄인 사람을 무죄로 잘못 판결내리는 것도 나쁜 실수이지만 무죄인 사람을 유죄로 판결내리는 것은 훨씬 더 나쁜 실수이다.\n",
    "이런 식으로 용납할 수 없는 실수 A가 발생하지 않도록 하려면, 다른 실수보다 실수 A가 발생할 때마다 더 큰 벌을 줘야 한다.\n",
    "예를 들어, loss 값에 제곱을 취하면 매우 나쁜 실수가 2배 더 매우 나쁜 실수가 되는데, 이러한 loss function은 큰 실수에는 더 큰 벌을 내린다는 뜻이 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Regularization\n",
    "\n",
    "regularization은 loss function에 반드시 포함되어야 하는 또 다른 loss 개념이다.\n",
    "만약 trainig data에 대한 loss만 신경쓴다면, 모델은 training data에 조금이라도 어긋나지 않기 위해 지나치게 복잡한 모델을 만들게 된다. training data만 달달 외운 복잡한 모델은 정작 가장 중요한 test data에 대한 generalization 능력이 떨어진다.\n",
    "training error에 비해 test error가 현저히 떨어지는 현상을 두고 \"모델이 overfitting이 되었다\"고 말한다.\n",
    "overfitting을 방지하려면 간단한 모델을 만들도록 유도해야 한다.\n",
    "regularization은 모델이 복잡할수록 regularization loss가 커지게 만들어서, 모델이 training data에만 지나치게 overfitting되는 것을 방지한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![L03-regualrization.png](images/L03-regualrization.png)\n",
    "\n",
    "그림과 같이 표준적인 loss function은 data loss와 regularization loss 이렇게 2가지 term을 포함한다.\n",
    "loss function을 나타내는 $L(W)$는 data loss term을 넣어서 training data에 fit하게 만들뿐만 아니라, regularization term도 추가해서 model이 overfitting 되지 않도록 더 간단한 파라미터 W를 선택하도록 유도한다.\n",
    "regularization term에 있는 lambda는 hyperparameter인데, 이는 data loss와 regularization loss의 trade-off를 의미한다.\n",
    "lambda 값이 커지면 regularization loss에 비중이 높아지는 만큼 상대적으로 data loss에 대한 비중은 낮아지기 때문이다.\n",
    "lambda는 모델을 tuning할 때 중요한 영향을 끼치는 hyperparameter이다.\n",
    "\n",
    "parameter $W$는 왜 간단할수록 좋을까?\n",
    "앞서 말한 내용에 따르면, 모델이 복잡할수록 test data에 대한 generalization 효과가 떨어진다고 했다.\n",
    "왜 그럴까?\n",
    "과학적 발견에서 쓰이는 핵심적인 아이디어인 Occam's Razor에서 근거를 찾을 수 있다.\n",
    "\"만약 관찰 데이터를 설명할 수 있는 모델이 여러 개 있다면, 더 간단한 모델을 선택해야 한다.\"\n",
    "왜냐하면 간단한 모델일수록 아직 나타나지 않은 새로운 데이터에 대해 일반화를 잘할 가능성이 더 높기 때문이다.\n",
    "regularization 연산은 regularization penalty(=loss)를 주는 방식으로 수행하고, weight가 작아지도록 유도하기 때문에 weight decay라고도 부른다.\n",
    "일반적으로 지나치게 복잡한 모델에 weight decay를 수행하면 test set에 대한 성능 향상이 일어난다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Different Types of Regularization\n",
    "\n",
    "![L03-types-of-regualrization.png](images/L03-types-of-regualrization.png)\n",
    "\n",
    "regularizaiton 연산을 하는 방법은 파라미터인 벡터 $w$의 길이를 구하고, 길이에 비례해서 regularization loss를 매기는 것이다.\n",
    "벡터 $w$의 길이를 구하는 방법은 L1-norm, L2-norm 등 다양하게 있으며 그에 따라 regularization의 종류도 달라진다.\n",
    "가장 많이 사용되는 regularization은 L2 regularization이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L2 regularization에는 2가지 키워드 Euclidean-norm과 spread가 있다.\n",
    "첫 번째 키워드는 벡터의 길이를 구하는 방법이다.\n",
    "L2-norm을 구하는 방법은 유클리디안 거리를 구하는 방법과 같기 때문에 Euclidean-norm이라고 부른다.\n",
    "가령, 벡터 $w$가 $(3, -2, 1)$일 경우 $||w||_ {2} = \\sqrt{(3^{2}+(-2)^{2}+1^{2})} = 3.742$가 된다.\n",
    "두 번째 키워드는 선호하는 벡터 $w$의 형태를 의미한다.\n",
    "L1 regularization과 비교할 때 각 원소의 단순 합이 똑같다면 L2 관점에서는 벡터 $w$의 원소값이 골고루 퍼져(spread) 있을수록 길이가 짧아지기 때문이다.\n",
    "정리하면, L2 regularization은 Euclidean-norm으로 파라미터 $W$를 penalize하며, 원소 값이 한쪽으로 쏠리기보다는 서로 비슷하게 spreaded out된 벡터 $w$를 선호한다.\n",
    "\n",
    "L1 regularization은 L2 regularization 다음으로 쓰인다.\n",
    "L1 regularization의 2가지 키워드는 Manhattan-norm과 sparse이다.\n",
    "Manhattan-norm은 L1 regularization에서 벡터의 길이를 구하는 방법을 뜻한다.\n",
    "미국 맨해튼은 직사각형 블록 단위로 이루어져 있는 도시라서 대각선으로 가로지르지 않고 가로세로로 이동한다.\n",
    "여기서 가로세로로 이동하는 방법이 L1 regularization에서 벡터의 길이를 구하는 방법과 같아서 Manhanttan-norm이라고 부른다.\n",
    "벡터 $w$가 $(3, -2, 1)$일 경우 $||w||_ {1} = \\Sigma{(3+(-2)+1)} = 2$가 된다.\n",
    "두 번째 키워드는 L2 regularization이 선호하는 벡터 $w$의 형태가 희소, 즉 sparse하다는 것을 뜻한다.\n",
    "벡터$w$의 형태가 sparse하다는 것은 특정 원소에 값이 몰려있고 나머지 여러 원소는 값이 0이 된다는 것을 뜻한다.\n",
    "원소 값에 0이 많다는 것은 값이 희박하다, 즉 sparse하다는 것을 뜻한다.\n",
    "정리하면, L1 regularization은 Manhattan-norm으로 파라미터 $W$를 penalize하며, 원소 값이 한쪽으로 쏠려 있고 나머지는 0이 되는 sparse한 벡터 $w$를 선호한다.\n",
    "\n",
    "> **Note:** norm : 벡터의 길이"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Meaning of L1 and L2 Regularization\n",
    "\n",
    "![L03-L2-regularization.png](images/L03-L2-regularization.png)\n",
    "\n",
    "linear classification에서는 input 데이터 $x$와 $W$를 내적(dot product)한다.\n",
    "그림에 있는 $w_1$을 택하든 $w_2$를 택하든 $x$와 내적하면 둘 다 1이 되기 때문에 결과는 같다.\n",
    "길이가 같은데 어떤 파라미터를 택해야 할까?\n",
    "이미지를 입력 받는 linear classification에서 $w$의 의미를 한 번 더 되새기고 다음으로 넘어가자.\n",
    "$w$는 $w$의 각 원소와 곱해지는 $x$의 각 픽셀값이 output class와 얼마나 일치하는지를 알려주는 값이었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) L2 regularization\n",
    "\n",
    "L2 regularization을 하면 $w_2$를 선택하게 된다.\n",
    "L2-norm은 제곱을 한다.\n",
    "$w_1$의 길이($1=1^{2} + 0^{2} + 0^{2} + 0^{2}$)보다 w2의 길이($0.25=0.25^{2} * 4$)가 더 작기 때문이다.\n",
    "L2 관점에서는 $w_2$처럼 벡터 $w$의 원소 값이 골고루 분포되어 있는 모델이 복잡도(regularization loss)가 더 작다고 생각한다.\n",
    "\n",
    "L2 regularzation이 linear classifier에 미치는 영향은 무엇일까?\n",
    "$w$가 골고루 퍼져있다는 것은 $w$와 곱(dot product)해질 input 벡터 $x$의 원소 또한 골고루 선택된다는 것을 뜻한다.\n",
    "즉, L2 regularization을 하면 벡터 $x$의 특정 element에만 의존하는 것이 아니라 벡터 $x$의 여러 element를 골고루 고려하는 모델을 선호하게 된다.\n",
    "\n",
    "#### (2) L1 regularization\n",
    "\n",
    "L1 regularization은 L2 regularization과 의미가 반대이다.\n",
    "L1 관점에서는 $w_1$처럼 벡터 $w$의 원소 값에 0이 많은 sparse한 모델이 복잡도(regularization loss)가 더 작다고 생각한다.\n",
    "\n",
    "> **Note**: 위 그림의 예시에서 L1 관점에서 w1의 길이($1=1+0+0+0$)와 w2의 길이($1=0.25+0.25+0.25+0.25$)는 같다.\n",
    "그림은 L2 관점에서 예시를 든 것이므로 L1 관점에서 $w_1$을 선호하려면 다른 예시로 생각해야 한다.\n",
    "\n",
    "$W$가 \"sparse하다\", 즉 \"zero 값이 많다\"는 것은 $W$와 곱(dot product)해질 입력값 벡터 $x$의 원소 또한 몇몇 원소는 제외하고 특정 원소만 선택된다는 것을 뜻한다.\n",
    "즉, L1 regularization은 벡터 $x$의 특정 element에 의존하는 모델을 선호하게 된다.\n",
    "\n",
    "> **Note**: L2와 L1을 언제 선택해야 하는지는 해결하고자 하는 문제와 데이터에 달려 있다 (problem and data dependent). 그런데 경험적으로는 거의 L2가 성능이 더 좋다고 알려져 있다.\n",
    "\n",
    "**끝.**\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
